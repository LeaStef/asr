model:
  encoder:
    input_size: 80
    hidden_size: 512
    memory_size: 256
    num_lmu_layers: 4
    theta: 1000
    dropout: 0.1
    use_fft_lmu: false
    use_attention: true
    num_attention_heads: 8
    use_downsampling: false
    downsample_factor: 2
  decoder:
    vocab_size: 32

data:
  dataset: "gigaspeech"    
  subset: "xs"
  save_dir: "./data/gigaspeech"
  sample_rate: 16000
  n_mels: 80
  max_seq_len: 500  # Reduced from 1000 for better performance
  augment: true
  num_workers: 16  # Increased from 4 for better I/O performance

training:
  batch_size: 16
  lr: 1e-4  # Reduced learning rate for stability
  max_epochs: 50
  patience: 10
  mixed_precision: true
  gradient_clip_norm: 0.5  # Reduced gradient clipping for stability
  accumulate_grad_batches: 1
  warmup_steps: 1000  # Add warmup for stability
  weight_decay: 0.01  # Add weight decay for regularization
  log_interval: 50  # Log every 50 steps to reduce overhead
