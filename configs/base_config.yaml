model:
  encoder:
    input_size: 80
    hidden_size: 384  # Optimized for speed vs accuracy balance
    memory_size: 192  # Proportional memory capacity
    num_lmu_layers: 3  # Balanced layer count for speed
    theta: 1000
    dropout: 0.1
    use_fft_lmu: false  # Disable FFT for variable sequence lengths
    use_attention: true
    num_attention_heads: 6  # Optimized attention heads
    use_downsampling: false  # Disable downsampling for stable training
    downsample_factor: 2
  decoder:
    vocab_size: 29

data:
  dataset: "gigaspeech"    
  subset: "xs"
  save_dir: "./data/gigaspeech"
  sample_rate: 16000
  n_mels: 80
  max_seq_len: 400  # Further reduced for faster processing
  augment: false  # Disable for initial fast training
  num_workers: 32  # Optimized for high-performance training

training:
  batch_size: 32  # Optimized for GPU memory utilization
  lr: 1.5e-3  # Slightly higher for faster convergence
  max_epochs: 50
  patience: 10  # Reduced patience for faster training cycles
  mixed_precision: true  # Essential for speed and memory
  gradient_clip_norm: 1.0  # Optimal clipping for stability
  accumulate_grad_batches: 1  # Direct batch processing for speed
  warmup_steps: 1500  # Balanced warmup for fast start
  weight_decay: 0.01  # Regularization
  log_interval: 10  # More frequent monitoring
