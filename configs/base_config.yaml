model:
  encoder:
    input_size: 80
    hidden_size: 512
    memory_size: 256
    num_lmu_layers: 4
    theta: 1000
    dropout: 0.1
    use_fft_lmu: false
  decoder:
    vocab_size: 29
  
data:
  dataset: "librispeech"
  subset: "clean-100"
  sample_rate: 16000
  n_mels: 80
  max_seq_len: 1000
  augment: true
  num_workers: 4
  
training:
  batch_size: 16
  lr: 1e-3
  max_epochs: 50
  patience: 10
  mixed_precision: true
  gradient_clip_norm: 1.0
  accumulate_grad_batches: 1