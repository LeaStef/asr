model:
  encoder:
    input_size: 80
    hidden_size: 256
    memory_size: 128
    num_lmu_layers: 2
    theta: 1000
    dropout: 0.1
    use_fft_lmu: false
    use_attention: true
    num_attention_heads: 4
    use_downsampling: false
    downsample_factor: 2
  decoder:
    vocab_size: 32

data:
  dataset: "gigaspeech"    
  subset: "xs"
  save_dir: "./data/gigaspeech"
  sample_rate: 16000
  n_mels: 80
  max_seq_len: 500  # Reduced from 1000 for better performance
  augment: true
  num_workers: 16  # Increased from 4 for better I/O performance

training:
  batch_size: 8
  lr: 5e-5  # Sweet spot learning rate
  max_epochs: 50
  patience: 15
  mixed_precision: false  # Disable for numerical stability
  gradient_clip_norm: 0.5  # Aggressive gradient clipping for stability
  accumulate_grad_batches: 1
  warmup_steps: 1000  # Moderate warmup
  weight_decay: 0.01  # Add weight decay for regularization
  log_interval: 20  # More frequent logging for monitoring
