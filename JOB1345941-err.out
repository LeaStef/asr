W0827 19:18:55.054000 255281 torch/distributed/run.py:785] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[rank3]: Traceback (most recent call last):
[rank3]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 451, in <module>
[rank3]:     exit_code = main()
[rank3]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 401, in main
[rank3]:     model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)
[rank3]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank3]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank3]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank3]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank3]: RuntimeError: DDP expects same model across all ranks, but Rank 3 has 60 params, while rank 0 has inconsistent 0 params.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 451, in <module>
[rank1]:     exit_code = main()
[rank1]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 401, in main
[rank1]:     model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)
[rank1]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank1]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank1]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank1]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank1]: RuntimeError: DDP expects same model across all ranks, but Rank 1 has 60 params, while rank 0 has inconsistent 0 params.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 451, in <module>
[rank0]:     exit_code = main()
[rank0]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 401, in main
[rank0]:     model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)
[rank0]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: RuntimeError: DDP expects same model across all ranks, but Rank 0 has 60 params, while rank 1 has inconsistent 0 params.
[rank2]: Traceback (most recent call last):
[rank2]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 451, in <module>
[rank2]:     exit_code = main()
[rank2]:   File "/u4/h6ly/asr/scripts/train_flexible.py", line 401, in main
[rank2]:     model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)
[rank2]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank2]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank2]:   File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank2]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank2]: RuntimeError: DDP expects same model across all ranks, but Rank 2 has 60 params, while rank 0 has inconsistent 0 params.
W0827 19:19:01.649000 255281 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 255300 closing signal SIGTERM
W0827 19:19:01.650000 255281 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 255301 closing signal SIGTERM
W0827 19:19:01.650000 255281 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 255303 closing signal SIGTERM
E0827 19:19:01.795000 255281 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 2 (pid: 255302) of binary: /u4/h6ly/asr/asr/bin/python
Traceback (most recent call last):
  File "/u4/h6ly/asr/asr/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/u4/h6ly/asr/asr/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
scripts/train_flexible.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-27_19:19:01
  host      : watgpu608.watgpu-domain.cs.uwaterloo.ca
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 255302)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
