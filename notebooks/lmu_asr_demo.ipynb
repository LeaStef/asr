{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMU-based ASR System: Complete Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow for training and using the LMU-based Automatic Speech Recognition system, from setup to inference.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Environment Setup**: Configure the environment and imports\n",
    "2. **Data Preparation**: Load and preprocess LibriSpeech dataset\n",
    "3. **Model Configuration**: Set up the LMU ASR model\n",
    "4. **Training**: Train the model with proper logging\n",
    "5. **Evaluation**: Evaluate model performance\n",
    "6. **Inference**: Run inference on new audio samples\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- PyTorch with CUDA support\n",
    "- All dependencies from requirements.txt\n",
    "- pytorch-lmu library (located in ../pytorch-lmu/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Set up plotting\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Print system information\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Project Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from config.config import Config, ModelConfig, DataConfig, TrainingConfig, create_config_from_dict\n",
    "from models.asr_model import create_model, LMUASRModel\n",
    "from data.dataset import create_dataloaders, HuggingFaceLibriSpeechDataset\n",
    "from data.preprocessing import AudioPreprocessor, TextPreprocessor, SpecAugment\n",
    "from training.trainer import Trainer\n",
    "from training.utils import (\n",
    "    decode_predictions, decode_targets, compute_wer, compute_cer,\n",
    "    count_parameters\n",
    ")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration for demo (smaller model for faster training)\n",
    "config = Config(\n",
    "    model=ModelConfig(\n",
    "        input_size=80,           # Mel spectrogram features\n",
    "        hidden_size=256,         # Reduced for demo\n",
    "        memory_size=128,         # Reduced for demo\n",
    "        num_lmu_layers=2,        # Reduced for demo\n",
    "        theta=1000.0,\n",
    "        dropout=0.1,\n",
    "        use_fft_lmu=False,       # Use standard LMU\n",
    "        vocab_size=29            # Will be updated based on actual vocab\n",
    "    ),\n",
    "    data=DataConfig(\n",
    "        dataset=\"librispeech\",\n",
    "        subset=\"clean\",\n",
    "        sample_rate=16000,\n",
    "        n_mels=80,\n",
    "        max_seq_len=800,         # Reduced for demo\n",
    "        augment=True,\n",
    "        num_workers=2            # Reduced for demo\n",
    "    ),\n",
    "    training=TrainingConfig(\n",
    "        batch_size=8,            # Reduced for demo\n",
    "        lr=1e-3,\n",
    "        max_epochs=3,            # Reduced for demo\n",
    "        patience=5,\n",
    "        mixed_precision=True,\n",
    "        gradient_clip_norm=1.0,\n",
    "        accumulate_grad_batches=1\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Configuration created:\")\n",
    "print(f\"  Model: {config.model.num_lmu_layers} LMU layers, {config.model.hidden_size} hidden units\")\n",
    "print(f\"  Data: {config.data.max_seq_len} max sequence length, batch size {config.training.batch_size}\")\n",
    "print(f\"  Training: {config.training.max_epochs} epochs, LR {config.training.lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "print(\"Loading LibriSpeech dataset...\")\n",
    "train_loader, val_loader, vocab = create_dataloaders(config.data, use_huggingface=True)\n",
    "\n",
    "# Update vocab size in config\n",
    "config.model.vocab_size = vocab['vocab_size']\n",
    "\n",
    "print(f\"✅ Data loaded successfully!\")\n",
    "print(f\"  Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"  Validation samples: {len(val_loader.dataset)}\")\n",
    "print(f\"  Vocabulary size: {vocab['vocab_size']}\")\n",
    "print(f\"  Batch size: {config.training.batch_size}\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "spectrograms, texts, input_lengths, target_lengths = sample_batch\n",
    "\n",
    "print(\"Sample batch shapes:\")\n",
    "print(f\"  Spectrograms: {spectrograms.shape}\")\n",
    "print(f\"  Texts: {texts.shape}\")\n",
    "print(f\"  Input lengths: {input_lengths.shape}\")\n",
    "print(f\"  Target lengths: {target_lengths.shape}\")\n",
    "\n",
    "# Visualize a spectrogram\n",
    "plt.figure(figsize=(12, 4))\n",
    "spec_sample = spectrograms[0, :input_lengths[0], :].numpy()\n",
    "plt.imshow(spec_sample.T, aspect='auto', origin='lower', cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title('Sample Mel Spectrogram')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Mel Frequency Bins')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show some text samples\n",
    "print(\"\\nSample texts:\")\n",
    "for i in range(min(3, len(texts))):\n",
    "    text_indices = texts[i][:target_lengths[i]].tolist()\n",
    "    decoded_text = ''.join([vocab['idx_to_char'][idx] for idx in text_indices if idx in vocab['idx_to_char']])\n",
    "    print(f\"  {i+1}: '{decoded_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Creation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "print(\"Creating LMU ASR model...\")\n",
    "model = create_model(config.model).to(device)\n",
    "\n",
    "# Analyze model\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Model size: {total_params * 4 / 1e6:.2f} MB\")\n",
    "\n",
    "# Test forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = spectrograms[:2].to(device)\n",
    "    test_lengths = input_lengths[:2].to(device)\n",
    "    \n",
    "    log_probs, memory_states = model(test_input, test_lengths)\n",
    "    print(f\"\\nForward pass test:\")\n",
    "    print(f\"  Input shape: {test_input.shape}\")\n",
    "    print(f\"  Output shape: {log_probs.shape}\")\n",
    "    print(f\"  Memory states: {len(memory_states)} layers\")\n",
    "    \n",
    "    # Test decoding\n",
    "    predictions = model.decode(log_probs, test_lengths)\n",
    "    print(f\"  Decoded predictions: {len(predictions)} sequences\")\n",
    "    \n",
    "    # Show sample predictions (before training)\n",
    "    pred_texts = decode_predictions(predictions, vocab)\n",
    "    print(f\"\\nSample predictions (before training):\")\n",
    "    for i, pred in enumerate(pred_texts[:2]):\n",
    "        print(f\"  {i+1}: '{pred}'\")\n",
    "\n",
    "print(\"\\n✅ Model ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "log_dir = './demo_logs'\n",
    "trainer = Trainer(model, config.training, device, log_dir)\n",
    "\n",
    "print(f\"Trainer created with log directory: {log_dir}\")\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {config.training.max_epochs}\")\n",
    "print(f\"  Batch size: {config.training.batch_size}\")\n",
    "print(f\"  Learning rate: {config.training.lr}\")\n",
    "print(f\"  Mixed precision: {config.training.mixed_precision}\")\n",
    "print(f\"  Early stopping patience: {config.training.patience}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"Note: This is a demo with reduced epochs and model size.\")\n",
    "print(\"For production use, increase epochs, model size, and dataset size.\")\n",
    "\n",
    "# Start training\n",
    "trainer.fit(train_loader, val_loader, vocab)\n",
    "\n",
    "print(\"\\n✅ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Metrics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and visualize training metrics\n",
    "metrics_path = os.path.join(log_dir, 'metrics', 'training_metrics.json')\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    # Plot training metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training and validation loss\n",
    "    if 'train_loss' in metrics and 'val_loss' in metrics:\n",
    "        axes[0, 0].plot(metrics['train_loss'], label='Training Loss', alpha=0.7)\n",
    "        axes[0, 0].plot(metrics['val_loss'], label='Validation Loss', alpha=0.7)\n",
    "        axes[0, 0].set_title('Training and Validation Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Word Error Rate\n",
    "    if 'val_wer' in metrics:\n",
    "        axes[0, 1].plot(metrics['val_wer'], label='Validation WER', color='red', alpha=0.7)\n",
    "        axes[0, 1].set_title('Word Error Rate')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('WER')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Character Error Rate\n",
    "    if 'val_cer' in metrics:\n",
    "        axes[1, 0].plot(metrics['val_cer'], label='Validation CER', color='green', alpha=0.7)\n",
    "        axes[1, 0].set_title('Character Error Rate')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('CER')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning Rate\n",
    "    if 'learning_rate' in metrics:\n",
    "        axes[1, 1].plot(metrics['learning_rate'], label='Learning Rate', color='purple', alpha=0.7)\n",
    "        axes[1, 1].set_title('Learning Rate Schedule')\n",
    "        axes[1, 1].set_xlabel('Step')\n",
    "        axes[1, 1].set_ylabel('Learning Rate')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"Final Training Metrics:\")\n",
    "    if 'val_loss' in metrics and metrics['val_loss']:\n",
    "        print(f\"  Final Validation Loss: {metrics['val_loss'][-1]:.4f}\")\n",
    "    if 'val_wer' in metrics and metrics['val_wer']:\n",
    "        print(f\"  Final Validation WER: {metrics['val_wer'][-1]:.4f}\")\n",
    "    if 'val_cer' in metrics and metrics['val_cer']:\n",
    "        print(f\"  Final Validation CER: {metrics['val_cer'][-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No metrics file found. Training may not have completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model if available\n",
    "best_model_path = os.path.join(log_dir, 'checkpoints', 'checkpoint_epoch_1_best.pt')\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Loading best model...\")\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "else:\n",
    "    print(\"Using current model state (best model checkpoint not found)\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating model on validation set...\")\n",
    "val_loss, val_wer, val_cer = trainer.validate(val_loader, vocab)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Loss: {val_loss:.4f}\")\n",
    "print(f\"  WER: {val_wer:.4f} ({val_wer*100:.2f}%)\")\n",
    "print(f\"  CER: {val_cer:.4f} ({val_cer*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inference Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on validation samples\n",
    "model.eval()\n",
    "num_examples = 5\n",
    "\n",
    "print(f\"Running inference on {num_examples} validation samples...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        if i >= 1:  # Only process first batch\n",
    "            break\n",
    "            \n",
    "        spectrograms, texts, input_lengths, target_lengths = batch\n",
    "        \n",
    "        # Move to device\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        texts = texts.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        log_probs, _ = model(spectrograms, input_lengths)\n",
    "        \n",
    "        # Decode predictions\n",
    "        predictions = model.decode(log_probs, input_lengths)\n",
    "        \n",
    "        # Convert to text\n",
    "        pred_texts = decode_predictions(predictions, vocab)\n",
    "        target_texts = decode_targets(texts, target_lengths, vocab)\n",
    "        \n",
    "        # Display results\n",
    "        for j in range(min(num_examples, len(pred_texts))):\n",
    "            print(f\"Example {j+1}:\")\n",
    "            print(f\"  Target:     '{target_texts[j]}'\")\n",
    "            print(f\"  Prediction: '{pred_texts[j]}'\")\n",
    "            \n",
    "            # Calculate individual WER and CER\n",
    "            if target_texts[j].strip():\n",
    "                individual_wer = compute_wer([pred_texts[j]], [target_texts[j]])\n",
    "                individual_cer = compute_cer([pred_texts[j]], [target_texts[j]])\n",
    "                print(f\"  WER: {individual_wer:.4f}, CER: {individual_cer:.4f}\")\n",
    "            print()\n",
    "            \n",
    "            if j + 1 >= num_examples:\n",
    "                break\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Custom Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(model, audio_processor, text_processor, audio_path_or_tensor, device):\n",
    "    \"\"\"\n",
    "    Transcribe audio using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained ASR model\n",
    "        audio_processor: AudioPreprocessor instance\n",
    "        text_processor: TextPreprocessor instance\n",
    "        audio_path_or_tensor: Path to audio file or audio tensor\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        transcription: Transcribed text\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process audio\n",
    "        if isinstance(audio_path_or_tensor, str):\n",
    "            # Load from file\n",
    "            features = audio_processor.preprocess_audio(audio_path_or_tensor)\n",
    "        else:\n",
    "            # Process tensor\n",
    "            features = audio_processor.extract_mel_features(audio_path_or_tensor)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        features = features.unsqueeze(0).to(device)\n",
    "        input_lengths = torch.tensor([features.shape[1]], device=device)\n",
    "        \n",
    "        # Forward pass\n",
    "        log_probs, _ = model(features, input_lengths)\n",
    "        \n",
    "        # Decode\n",
    "        predictions = model.decode(log_probs, input_lengths)\n",
    "        \n",
    "        # Convert to text\n",
    "        vocab_dict = {\n",
    "            'idx_to_char': text_processor.idx_to_char,\n",
    "            'char_to_idx': text_processor.char_to_idx,\n",
    "            'vocab_size': text_processor.get_vocab_size(),\n",
    "            'blank_token_id': text_processor.get_blank_token_id()\n",
    "        }\n",
    "        \n",
    "        transcription = decode_predictions(predictions, vocab_dict)[0]\n",
    "        \n",
    "        return transcription\n",
    "\n",
    "# Create processors for inference\n",
    "audio_processor = AudioPreprocessor(\n",
    "    sample_rate=config.data.sample_rate,\n",
    "    n_mels=config.data.n_mels,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "text_processor = TextPreprocessor()\n",
    "\n",
    "print(\"✅ Custom inference function ready!\")\n",
    "print(\"You can now use transcribe_audio() to transcribe new audio files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze model architecture\n",
    "print(\"Model Architecture Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Count parameters by component\n",
    "encoder_params = sum(p.numel() for p in model.encoder.parameters())\n",
    "decoder_params = sum(p.numel() for p in model.decoder.parameters())\n",
    "\n",
    "print(f\"Encoder parameters: {encoder_params:,} ({encoder_params/total_params*100:.1f}%)\")\n",
    "print(f\"Decoder parameters: {decoder_params:,} ({decoder_params/total_params*100:.1f}%)\")\n",
    "\n",
    "# Analyze LMU layers\n",
    "print(f\"\\nLMU Layer Configuration:\")\n",
    "print(f\"  Number of layers: {len(model.encoder.lmu_layers)}\")\n",
    "print(f\"  Hidden size: {model.encoder.hidden_size}\")\n",
    "print(f\"  Memory size: {model.encoder.memory_size}\")\n",
    "print(f\"  Theta (timescale): {model.encoder.theta}\")\n",
    "print(f\"  Dropout: {model.encoder.dropout}\")\n",
    "\n",
    "# Vocabulary analysis\n",
    "print(f\"\\nVocabulary Analysis:\")\n",
    "print(f\"  Vocabulary size: {vocab['vocab_size']}\")\n",
    "print(f\"  Characters: {list(vocab['char_to_idx'].keys())}\")\n",
    "print(f\"  Blank token ID: {vocab['blank_token_id']}\")\n",
    "\n",
    "# Memory usage (if on GPU)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"  Cached: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = os.path.join(log_dir, 'final_model.pt')\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'vocab': vocab,\n",
    "    'model_params': total_params,\n",
    "    'final_metrics': {\n",
    "        'val_loss': val_loss,\n",
    "        'val_wer': val_wer,\n",
    "        'val_cer': val_cer\n",
    "    }\n",
    "}, final_model_path)\n",
    "\n",
    "print(f\"✅ Final model saved to: {final_model_path}\")\n",
    "\n",
    "# Demonstrate loading\n",
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"Load a trained model for inference.\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model\n",
    "    config = checkpoint['config']\n",
    "    model = create_model(config.model).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    vocab = checkpoint['vocab']\n",
    "    \n",
    "    return model, config, vocab\n",
    "\n",
    "# Test loading\n",
    "loaded_model, loaded_config, loaded_vocab = load_trained_model(final_model_path, device)\n",
    "print(f\"✅ Model loaded successfully!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in loaded_model.parameters()):,}\")\n",
    "print(f\"  Vocabulary size: {loaded_vocab['vocab_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Next Steps and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Improvements\n",
    "\n",
    "To improve this demo model for production use:\n",
    "\n",
    "1. **Scale up the model**:\n",
    "   - Increase `hidden_size` to 512-1024\n",
    "   - Increase `memory_size` to 256-512\n",
    "   - Use 4-8 LMU layers\n",
    "   - Increase `max_seq_len` to 1000-2000\n",
    "\n",
    "2. **Extended training**:\n",
    "   - Train for 50-100 epochs\n",
    "   - Use larger batch sizes (16-32)\n",
    "   - Implement learning rate scheduling\n",
    "   - Use the full LibriSpeech dataset\n",
    "\n",
    "3. **Advanced techniques**:\n",
    "   - Language model integration\n",
    "   - Beam search decoding\n",
    "   - Sub-word tokenization\n",
    "   - Multi-task learning\n",
    "\n",
    "4. **Distributed training**:\n",
    "   - Use multiple GPUs with `train_distributed.py`\n",
    "   - Implement gradient accumulation\n",
    "   - Use advanced optimizers (AdamW, etc.)\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - Test on multiple datasets\n",
    "   - Implement confidence scoring\n",
    "   - Add real-time inference capabilities\n",
    "\n",
    "### Usage Examples\n",
    "\n",
    "```python\n",
    "# For production training\n",
    "python scripts/train.py --config-name=base_config\n",
    "\n",
    "# For distributed training\n",
    "torchrun --nproc_per_node=4 scripts/train_distributed.py\n",
    "\n",
    "# For evaluation\n",
    "python scripts/evaluate.py --checkpoint_path=path/to/checkpoint.pt\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **LMU layers** provide effective temporal modeling for speech\n",
    "2. **CTC loss** enables alignment-free training\n",
    "3. **Mixed precision** training reduces memory usage\n",
    "4. **Proper data augmentation** improves robustness\n",
    "5. **Distributed training** scales to larger models and datasets\n",
    "\n",
    "This demo provides a solid foundation for building production-ready ASR systems with LMU architecture!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}